{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Dependencies\n",
    "from Pipeline_Support.ETL_MasterFunction import etl_master\n",
    "from Pipeline_Support.DimensionalQueries import dimquery\n",
    "from Pipeline_Support.FactSnapshot import create_fact_snapshot\n",
    "from Pipeline_Support.FTSUpload import upload_fact_table\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populating Database\n",
    "First we need to generate synthetic data from Mockaroo for our defined database schema and upload the datasets on a GitHub repo which we'll be using as a proxy for a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Pipeline_Support/DataGen.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL\n",
    "The following operations are done in the ETL Pipeline:\n",
    "* It fetches the datasets (csv tables) from the GitHub Repo and stores all of them as dataframes in a dictionary.\n",
    "* Then it treats for the missing values in each of the dataframes: some of the features like rent_amount, sale_amount, commission_value, commission_rate is filled logically, other numeric features are imputed through KNN imputer (each dataframe separately), and finally categorical features are filled with modal value.\n",
    "* Then it corrects the data types for various columns in each dataframe where needed.\n",
    "* Then the star schema is created as per the blueprint already defined:\n",
    "    * First Date dimension is created which has dates generated from 01/01/2021 to 31/12/2023 with a unique data key.\n",
    "    * Then Location dimension is generated and location id from the oltp table is kept along with a unique location.\n",
    "    * Similarly, Agent, Property Details, and Listing dimensions are generated.\n",
    "    * Then the fact table is generated by doing multiple joins (left dataframe merges) to get a big dataframe with all the necessary features required to create facts and then once facts are created unnecessary columns are dropped. Finally, fact dataframe is merged with each dimensional dataframe based on the oltp ids kept in the dimensions to get dimension foreign keys in the fact table. Lastly, oltp ids are dropped from dimensions.\n",
    "    * *Note: This is a brief summary that doesn't cover every detail of the ETL pipeline. For detailed understanding, check ETL_MasterFunction.py & ETL_SupportFunctions.py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Fetching from both CSV + PostgreSQL...\n",
      "âœ… Data ingestion complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Github\\DWM-Project\\E2E_DWH_Pipeline\\Pipeline_Support\\ETL_SupportFunctions.py:181: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(mode_value, inplace=True)\n",
      "d:\\Github\\DWM-Project\\E2E_DWH_Pipeline\\Pipeline_Support\\ETL_SupportFunctions.py:181: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(mode_value, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Missing values filled.\n",
      "âœ… Data types corrected.\n",
      "âœ… Star schema generated successfully.\n",
      "âœ… All CSVs saved successfully in E2E_DWH_Pipeline folders.\n",
      "â¬†ï¸ Loading tables into PostgreSQL database...\n",
      "âœ… Data successfully loaded into PostgreSQL!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"D:\\Github\\DWM-Project\")\n",
    "\n",
    "Dim_Date, Dim_Location, Dim_Agent, Dim_PropertyDetails, Dim_Listing, Fact_Transaction = etl_master(\n",
    "    source=\"hybrid\",\n",
    "    db_params={\n",
    "        'user': 'postgres',\n",
    "        'password': 'asifa123',\n",
    "        'host': 'localhost',\n",
    "        'port': 5432,\n",
    "        'db_name': 'Real-Estate-Management'\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Analytical Requirements of the Business\n",
    "**The following analytical queries are answered through our created data mart (star schema). For further details, check DimensionalQueries.py:**\n",
    "\n",
    "1- What was the total commission generated through agents handling rental listings in the first quarter of 2021?\n",
    "\n",
    "2- What was the average time taken to close a listing by agents handling sale listings in all four quarters of 2022?\n",
    "\n",
    "3- What was average commission rate of the top 5 cities with the most sale transactions in 2023?\n",
    "\n",
    "4- What was the average time spent in negotiations by agents for the top 3 states with the most rental transactions in 2022 and 2023?\n",
    "\n",
    "5- What was the average commission generated by middle-aged male and female agents in 2022?\n",
    "\n",
    "6- What was the average transaction value for Broker Associate in the fourth quarter of 2023? \n",
    "\n",
    "7- What was average number of transactions handled by agents who've joined in the past two years?\n",
    "\n",
    "8- What was the average time taken to close a sale listing where property size was greater 3000 sqft and had more than 4 bedrooms in 2021?\n",
    "\n",
    "9- What is the average maintenance cost and demand price discount for properties which are built in last two decades?\n",
    "\n",
    "10- What is the average time taken to close a listing which has a property in a poor condition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1\n",
      "+---+--+\n",
      "| 0 |  |\n",
      "+---+--+ \n",
      "\n",
      "\n",
      "Query 2\n",
      "+---+--+\n",
      "| 0 |  |\n",
      "+---+--+ \n",
      "\n",
      "\n",
      "Query 3\n",
      "+---+---------------+-------+\n",
      "| 0 | San Francisco | 2.125 |\n",
      "| 1 | Oklahoma City | 1.32  |\n",
      "| 2 | New York      | 2.125 |\n",
      "| 3 | Los Angeles   | 1.615 |\n",
      "| 4 | Huntsville    | 1.615 |\n",
      "+---+---------------+-------+ \n",
      "\n",
      "\n",
      "Query 4\n",
      " \n",
      "\n",
      "\n",
      "Query 5\n",
      " \n",
      "\n",
      "\n",
      "Query 6\n",
      "+---+--+\n",
      "| 0 |  |\n",
      "+---+--+ \n",
      "\n",
      "\n",
      "Query 7\n",
      "+---+--+\n",
      "| 0 |  |\n",
      "+---+--+ \n",
      "\n",
      "\n",
      "Query 8\n",
      "+---+--+\n",
      "| 0 |  |\n",
      "+---+--+ \n",
      "\n",
      "\n",
      "Query 9\n",
      "+---+---------+---------+\n",
      "| 0 | 2244.17 | 36.4927 |\n",
      "+---+---------+---------+ \n",
      "\n",
      "\n",
      "Query 10\n",
      "+---+-----+\n",
      "| 0 | 455 |\n",
      "+---+-----+ \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dimquery(Dim_Date=Dim_Date,Dim_Location=Dim_Location,Dim_Agent=Dim_Agent,Dim_PropertyDetails=Dim_PropertyDetails,Dim_Listing=Dim_Listing,Fact_Transaction=Fact_Transaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fact Table Snapshot\n",
    "Fact table snapshot is created by simply joining the fact table with dimensional tables based on the dimensional keys and which gives in return a big dataframe that has all the dimensional attributes of all dimensional tables within the fact table (based on their respective keys which were foreign keys in the fact table) along with dimensional keys and finally dimensional keys are then dropped from the fact table.\n",
    "\n",
    "Then the fact table snapshot is uploaded on the GitHub repo as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fact Snapshot saved successfully at:\n",
      "D:\\Github\\DWM-Project\\E2E_DWH_Pipeline\\Fact Table Snapshot\\Fact_Snapshot.csv\n",
      "âœ… Fact_Transaction table saved at:\n",
      "D:\\Github\\DWM-Project\\E2E_DWH_Pipeline\\Fact Table Snapshot\\Fact_Transaction.csv\n"
     ]
    }
   ],
   "source": [
    "Fact_Snap=create_fact_snapshot(Dim_Date=Dim_Date,Dim_Location=Dim_Location,Dim_Agent=Dim_Agent,Dim_PropertyDetails=Dim_PropertyDetails,Dim_Listing=Dim_Listing,Fact_Transaction=Fact_Transaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ§© Type check before function call:\")\n",
    "print(\"Dim_Date:\", type(Dim_Date))\n",
    "print(\"Dim_Location:\", type(Dim_Location))\n",
    "print(\"Dim_Agent:\", type(Dim_Agent))\n",
    "print(\"Dim_PropertyDetails:\", type(Dim_PropertyDetails))\n",
    "print(\"Dim_Listing:\", type(Dim_Listing))\n",
    "print(\"Fact_Transaction:\", type(Fact_Transaction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "upload_fact_table(file_content=Fact_Snap.to_csv(index=False),file_name='FactSnapshot.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power BI Dashboard\n",
    "* First we've connected the Power BI Desktop App with the fact table snapshot uploaded on the GitHub Repo: Get data > Web > \"raw.githubusercontent link of fact table snapshot.\"\n",
    "* Designed the dashboard on the Desktop App and published it.\n",
    "* Opened the published dashboard online and published it to web to get a public URL for our dashboard: File > Embed Report > Publish to web.\n",
    "* Then we went to Pabbly Connect and setup a workflow which will automate the data refresh of our dashboard: **Action** is refresh Power BI dashboard dataset when **Trigger** whenever a commit is pushed on the GitHub repo is trigerred.\n",
    "\n",
    "In this way when we will run the pipeline again with new data, fact table snapshot csv file on the repo would be updated with the new csv file and our dashboard on the public URL will update itselfÂ withÂ the newÂ data!\n",
    "\n",
    "Also, important to note that the dashboard also shows annual change in the scorecards. For instance, if we select year 2021 and 2022 then it'll show the percentage change in the scorecards between those two years. By default, it shows the change between the max year and the preceeding year. Also, naturally if we just select one year then no change is displayed.\n",
    "\n",
    "**Dashboard URL : https://app.powerbi.com/view?r=eyJrIjoiZjYyOWQxMWItMGFmNi00M2QyLWIzYWItMDYxOTc3ZjBmNmYwIiwidCI6ImZlZTNiOTE2LTAxYzEtNDk4Ny1hNjQ2LWUxOTM0MzJiOWVhYSIsImMiOjl9**\n",
    "\n",
    "*Note: The URL here won't show the dashboard for years 2021-2023 anymore and instead show the dashboard for the years 2022-2024 as we've updated the dashboard below in the subsequent iteration of the pipeline with new data. However, you may refer to the snippet below for past reference.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path='Dashboard Snips/2021-2023.png'\n",
    "Image(filename=img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating Database\n",
    "In the first iteration of our pipeline:\n",
    "* It connected to the data sources.\n",
    "* Performed ETL operations to create a data mart from 1st Jan 2021 to 31st Dec 2023 based on Star Schema Dimensional Modelling.\n",
    "* Addressed potential business analytical queries.\n",
    "* Created & exported fact table snapshot for Power BI Dashboard.\n",
    "* Finally, the data mart is now visualized on the Power BI Dashboard.\n",
    "\n",
    "**However, what if year 2024 has ended and we've new data in our database for that year and we want to update our data mart and the dashboard?**\n",
    "\n",
    "Suppose that the real-estate management company updates the data mart every year and makes each iteration of the data mart for past 3 years of data and for each iteration also makes a comparison dashboard which compares the newly added year in the updated data mart with the last year of previous data mart. Further, suppose that our first iteration of the pipeline above was the creation of this data mart. So, the data mart was created first time at the end of 2023 and had data for 2021-2023. Then after year 2024, the data mart was updated with new data which was from 2022 to 2024. Also, a comparison dashboard was created which compared the years 2023 and 2024.\n",
    "\n",
    "To simulate this, we would first update our mockaroo schemas so that it generates data specifically for 2024 (200 rows) and append the generated data with our already existing data (1000 rows) on the database (GitHub Repo) by re-running the data generation script. Then, we would make the following minor tweaks to our ETL process and re-run the whole pipeline as before to get the updated Dashboard.\n",
    "* Filter the transactions from 01/01/2022 to 31/12/2024 instead of 01/01/2021 to 31/12/2023.\n",
    "* Create data dimension from 01/01/2022 to 31/12/2024 instead of 01/01/2021 to 31/12/2023.\n",
    "\n",
    "*Note: For both above changes we only need to modify the start_date & end_date parameters of the ETL master function.*\n",
    "\n",
    "Because of the Pabbly Connect workflow, we only need to re-run the complete pipeline in this second iteration, when new data is appended in the database, which will result in updating the fact table snapshot on the GitHub repo and this will trigger the dashboard to refresh with new dataset and show visuals for the updated data mart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Pipeline_Support/DataGen.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL\n",
    "The following operations are done in the ETL Pipeline:\n",
    "* It fetches the datasets (csv tables) from the GitHub Repo and stores all of them as dataframes in a dictionary.\n",
    "* Then it treats for the missing values in each of the dataframes: some of the features like rent_amount, sale_amount, commission_value, commission_rate is filled logically, other numeric features are imputed through KNN imputer (each dataframe separately), and finally categorical features are filled with modal value.\n",
    "* Then it corrects the data types for various columns in each dataframe where needed.\n",
    "* Then the star schema is created as per the blueprint already defined:\n",
    "    * First Date dimension is created which has dates generated from 01/01/2022 to 31/12/2024 with a unique data key.\n",
    "    * Then Location dimension is generated and location id from the oltp table is kept along with a unique location.\n",
    "    * Similarly, Agent, Property Details, and Listing dimensions are generated.\n",
    "    * Then the fact table is generated by doing multiple joins (left dataframe merges) to get a big dataframe with all the necessary features required to create facts and then once facts are created unnecessary columns are dropped. Finally, fact dataframe is merged with each dimensional dataframe based on the oltp ids kept in the dimensions to get dimension foreign keys in the fact table. Lastly, oltp ids are dropped from dimensions.\n",
    "    * *Note: This is a brief summary that doesn't cover every detail of the ETL pipeline. For detailed understanding, check ETL_MasterFunction.py & ETL_SupportFunctions.py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dim_Date, Dim_Location, Dim_Agent, Dim_PropertyDetails, Dim_Listing, Fact_Transaction=etl_master()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Analytical Requirements of the Business\n",
    "**The following analytical queries are answered through our created data mart (star schema). For further details, check DimensionalQueries.py:**\t\t\t\n",
    "\n",
    "1- What was the total commission generated through agents handling rental listings in the first quarter of 2022?\n",
    "\n",
    "2- What was the average time taken to close a listing by agents handling sale listings in all four quarters of 2022?\n",
    "\n",
    "3- What was average commission rate of the top 5 cities with the most sale transactions in 2023?\n",
    "\n",
    "4- What was the average time spent in negotiations by agents for the top 3 states with the most rental transactions in 2022 and 2023?\n",
    "\n",
    "5- What was the average commission generated by middle-aged male and female agents in 2022?\n",
    "\n",
    "6- What was the average transaction value for Broker Associate in the fourth quarter of 2023? \n",
    "\n",
    "7- What was average number of transactions handled by agents who've joined in the past two years?\n",
    "\n",
    "8- What was the average time taken to close a sale listing where property size was greater 3000 sqft and had more than 4 bedrooms in 2022?\n",
    "\n",
    "9- What is the average maintenance cost and demand price discount for properties which are built in last two decades?\n",
    "\n",
    "10- What is the average time taken to close a listing which has a property in a poor condition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimquery(Dim_Date=Dim_Date,Dim_Location=Dim_Location,Dim_Agent=Dim_Agent,Dim_PropertyDetails=Dim_PropertyDetails,Dim_Listing=Dim_Listing,Fact_Transaction=Fact_Transaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fact Table Snapshot\n",
    "Fact table snapshot is created by simply joining the fact table with dimensional tables based on the dimensional keys and which gives in return a big dataframe that has all the dimensional attributes of all dimensional tables within the fact table (based on their respective keys which were foreign keys in the fact table) along with dimensional keys and finally dimensional keys are then dropped from the fact table.\n",
    "\n",
    "Then the fact table snapshot is uploaded on the GitHub repo as a csv file which in this second iteration would simply replace the previous csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fact_Snap=create_fact_snapshot(Dim_Date=Dim_Date,Dim_Location=Dim_Location,Dim_Agent=Dim_Agent,Dim_PropertyDetails=Dim_PropertyDetails,Dim_Listing=Dim_Listing,Fact_Transaction=Fact_Transaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_fact_table(file_content=Fact_Snap.to_csv(index=False),file_name='FactSnapshot.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power BI Dashboard\n",
    "Now that we have ran the pipeline again because new data is available in the database and it is time for the annual update of our data mart, already existing fact table snapshot csv file on the GitHub repo is replaced with the updated fact table snapshot and our dashboard on the public URL has updated itselfÂ withÂ the newÂ data. This is because of the Pabbly Connect workflow which refreshes the Power BI dashboard when triggered by a commit push on the GitHub repo (commit in this case is the replacement of the fact table snapshot csv file).\n",
    "\n",
    "Also, important to note that the dashboard also shows annual change in the scorecards. For instance, if we select year 2022 and 2023 then it'll show the percentage change in the scorecards between those two years. By default, it shows the change between the max year and the preceeding year. Also, naturally if we just select one year then no change is displayed.\n",
    "\n",
    "**Dashboard URL : https://app.powerbi.com/view?r=eyJrIjoiZjYyOWQxMWItMGFmNi00M2QyLWIzYWItMDYxOTc3ZjBmNmYwIiwidCI6ImZlZTNiOTE2LTAxYzEtNDk4Ny1hNjQ2LWUxOTM0MzJiOWVhYSIsImMiOjl9**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path='Dashboard Snips/2022-2024.png'\n",
    "Image(filename=img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pabbly Connect Worflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path='Pabbly Connect Workflow.png'\n",
    "Image(filename=img_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
